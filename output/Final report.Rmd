---
title: "Report Capstone Project"
author: "Marazzini Matilde"
date: "2022-12-20"
output:
  pdf_document: default
  html_document: default
---

# The Victory of Giorgia Meloni described by foreign press - a sentiment analysis

## INTRODUCTION

How was the victory of Giorgia Meloni depicted by the foreign press? It is well known that after Giorgia Meloni's victory in the elections many journalists wrote scandalous and exaggerated headlines, going so far as to liken the Fratelli di Italia party and its leader to fascism and Mussolini. The following study aims to analyze the sentiment of the foreign press regarding Meloni's election victory.

### Method

To pursue this objective, articles about the premier's victory published on Sept. 26 by the BBC, CNN, New York Times and The Guardian were analyzed. These articles were chosen because they were the first articles from major journals that resulted from the query "Giorgia Meloni won the election," meaning that, according to the SEO principles, they were probably the most read articles associated with this query.

Sentiment analysis was conducted on a word-level, meaning that the texts were fragmented and analyzed on the basis of individual words, operating both a generic analysis and a more specific one on each newspaper, performing a comparison between the four different sources. It should also be specified that a lexicon-based method was used, focusing on the creation of positive and negative word lists.

# DEVELOPMENT OF THE PROJECT

## Scraping and download of the texts

To achieve my goal, as first thing I needed to scrape the text of the articles from the web. So, I firstly checked the robots.txt on the websites and, as I saw that I could download the texts, I started to open all the packages I thought I would need for my report:

```{r}
#preparation
library(tidyverse)
library(rvest)
library(httr)
library(curl)
library(here)
library(stringr)
library(RCurl)
library(dplyr)
library(tidytext)
library(ggplot2)
library(tidyr)
library(transforEmotion)
```

Then, I proceeded with the download of the texts. Since the sentences were put in as different variables, I unified them in a unique text.

```{r}
#DOWNLOAD BBC
x <- read_html("https://www.bbc.com/news/world-europe-63029909") %>%
  html_elements(css = ".eq5iqo00 , #main-heading")
  
BBC <- html_text(x, trim = FALSE)

#I unify all the split senteces into one text
uni_BBC <- str_c(BBC[1], BBC[3], BBC[4], BBC[5], BBC[6], BBC[7], BBC[8], BBC[9], BBC[10], BBC[11], BBC[12], BBC[13], BBC[14], BBC[15], BBC[16], BBC[17], BBC[18], BBC[19], BBC[20], BBC[21], BBC[22], BBC[23], BBC[24], BBC[25], BBC[26], BBC[27], BBC[28], BBC[29], BBC[30], BBC[31], sep = "", collapse = NULL)
view(uni_BBC)

#DOWNLOAD NYT
y <- read_html("https://www.nytimes.com/2022/09/25/world/europe/italy-meloni-prime-minister.html") %>%
  html_elements(css = ".evys1bk0 , .css-by6x3t , .e1h9rw200")

NYT <- html_text(y, trim = FALSE)
view(NYT)

#I unified the split senteces into one text
uni_NYT <- str_c(NYT[1], NYT[3], NYT[4], NYT[5], NYT[6], NYT[7], NYT[8], NYT[9], NYT[10], NYT[11], NYT[12], NYT[13], NYT[14], NYT[15], NYT[16], NYT[17], NYT[18], NYT[19], NYT[20], NYT[21], NYT[22], NYT[23], NYT[24], NYT[25], NYT[26], NYT[27], NYT[28], NYT[29], NYT[30], NYT[31], NYT[32], NYT[33], NYT[34], NYT[35], NYT[36], NYT[37], sep = "", collapse = NULL)
view(uni_NYT)

#CNN
z <- read_html("https://edition.cnn.com/2022/09/25/europe/italy-election-results-intl/index.html") %>%
  html_elements(css = ".inline-placeholder")

CNN <- html_text(z, trim = FALSE)
view(CNN)

#I unified all the split senteces into one text
uni_CNN <- str_c(CNN[1], CNN[4], CNN[5], CNN[6], CNN[7], CNN[8], CNN[9], CNN[10], CNN[11], CNN[12], CNN[13], CNN[14], CNN[15], CNN[16], CNN[17], CNN[18], CNN[19], CNN[20], CNN[21], CNN[22], CNN[23], CNN[24], CNN[25], CNN[26], CNN[27], CNN[28], CNN[29], CNN[30], CNN[31], CNN[32], CNN[33], CNN[34], CNN[35], CNN[36], sep = "", collapse = NULL)
view(uni_CNN)

#GUARDIAN
r <- read_html("https://www.theguardian.com/world/2022/sep/25/italy-elections-exit-polls-point-to-victory-for-coalition-led-by-far-right-giorgia-meloni") %>%
  html_elements(css = "p , .dcr-y70mar")
head(r)

Grd <- html_text(r, trim = FALSE)
view(Grd)

#unify in one text
uni_Grd <- str_c(Grd[1], Grd[2], Grd[3], Grd[4], Grd[5], Grd[6], Grd[7], Grd[8], Grd[9], Grd[10], Grd[11], Grd[12], Grd[13], Grd[14], Grd[15], Grd[16], Grd[17], Grd[18], Grd[19], Grd[20], Grd[21], Grd[22], Grd[23], Grd[24], Grd[25], Grd[26], Grd[27], Grd[28], Grd[29], Grd[30], Grd[31], Grd[32], Grd[33], Grd[34], sep = "", collapse = NULL)
view(uni_Grd)
```

Then I created a data frame and a vector, so that for the next tasks of my project I would have them already ready to use:

```{r}
#create a dataframe
 dat <- data.frame(
  var1 = c("BBC", "NYT", "CNN", "Guardian"), 
  var2 = c(uni_BBC, uni_NYT, uni_CNN, uni_Grd)
)
dat
view(dat)

#create a vector
vector <- c(uni_BBC, uni_CNN, uni_Grd, uni_NYT)
```

## Most frequent words

To start to have an idea of the sentiment, it is good to have an insight of the most common words present in the texts. But before doing this, it is necessary to eliminate words that are frequent but have no meaning for the sentiment analysis, as the articles and the punctuation. So I used the package `tm` to eliminate these words and create a data frame with the frequency of the most common words.

```{r}
library(tm)
docs <- Corpus(VectorSource(vector))

docs <- docs %>%
  tm_map(removeNumbers) %>%
  tm_map(removePunctuation) %>%
  tm_map(stripWhitespace)
docs <- tm_map(docs, content_transformer(tolower))
docs <- tm_map(docs, removeWords, stopwords("english"))

dtm <- TermDocumentMatrix(docs) 
matrix <- as.matrix(dtm) 
words <- sort(rowSums(matrix),decreasing=TRUE) 
df <- data.frame(word = names(words),freq=words) #data frame with words and frequency
head(df)
```

To have a better visual representation of the most frequent words, I decided to create a wordcloud, which is more impactful.

```{r, results = 'asis'}
library(wordcloud2)
wordcloud2(data=df, size=0.4, color='random-dark')
```

As we could expect, the most frequent words are all linked with the field of politics but they all have a neutral meaning, like *party*, *government*, *minister*, *coalition*, *vote*...

Then, I wanted to analyze the most frequent words inside each article to test if the journals had different approaches. To do so, I needed first to create a single data frame on a word level for each journal:

```{r}
#FREQUENCY BBC
docs_bbc <- Corpus(VectorSource(BBC))

docs_bbc <- docs_bbc %>%
  tm_map(removeNumbers) %>%
  tm_map(removePunctuation) %>%
  tm_map(stripWhitespace)
docs_bbc <- tm_map(docs_bbc, content_transformer(tolower))
docs_bbc <- tm_map(docs_bbc, removeWords, stopwords("english"))

dtm_bbc <- TermDocumentMatrix(docs_bbc) 
matrix_bbc <- as.matrix(dtm_bbc) 
words_bbc <- sort(rowSums(matrix_bbc),decreasing=TRUE) 
df_bbc <- data.frame(word = names(words_bbc),freq=words_bbc)


#MOST FREQUENT WORDS NYT
docs_nyt <- Corpus(VectorSource(NYT))

docs_nyt <- docs_nyt %>%
  tm_map(removeNumbers) %>%
  tm_map(removePunctuation) %>%
  tm_map(stripWhitespace)
docs_nyt <- tm_map(docs_nyt, content_transformer(tolower))
docs_nyt <- tm_map(docs_nyt, removeWords, stopwords("english"))

dtm_nyt <- TermDocumentMatrix(docs_nyt) 
matrix_nyt <- as.matrix(dtm_nyt) 
words_nyt <- sort(rowSums(matrix_nyt),decreasing=TRUE) 
df_nyt <- data.frame(word = names(words_nyt),freq=words_nyt)
df_nyt <- df_nyt[!(df_nyt$word=="—" | df_nyt$word=="“"),]
df_nyt

#MOST FREQUENT WORDS CNN
docs_cnn <- Corpus(VectorSource(CNN))

docs_cnn <- docs_cnn %>%
  tm_map(removeNumbers) %>%
  tm_map(removePunctuation) %>%
  tm_map(stripWhitespace)
docs_cnn <- tm_map(docs_cnn, content_transformer(tolower))
docs_cnn <- tm_map(docs_cnn, removeWords, stopwords("english"))

dtm_cnn <- TermDocumentMatrix(docs_cnn) 
matrix_cnn <- as.matrix(dtm_cnn) 
words_cnn <- sort(rowSums(matrix_cnn),decreasing=TRUE) 
df_cnn <- data.frame(word = names(words_cnn),freq=words_cnn)
df_cnn <- df_cnn[!(df_cnn$word=="“"| df_cnn$word=="–"),]
df_cnn


#MOST FREQUENT WORDS GUARDIAN
docs_grd <- Corpus(VectorSource(Grd))

docs_grd <- docs_grd %>%
  tm_map(removeNumbers) %>%
  tm_map(removePunctuation) %>%
  tm_map(stripWhitespace)
docs_grd <- tm_map(docs_grd, content_transformer(tolower))
docs_grd <- tm_map(docs_grd, removeWords, stopwords("english"))

dtm_grd <- TermDocumentMatrix(docs_grd) 
matrix_grd <- as.matrix(dtm_grd) 
words_grd <- sort(rowSums(matrix_grd),decreasing=TRUE) 
df_grd <- data.frame(word = names(words_grd),freq=words_grd)
df_grd <- df_grd[!(df_grd$word=="“"),]
df_grd
```

At this point, to visualize the differences I created 4 bar charts for each journal, showing only the words that occur at least seven times:

```{r, results='asis'}
df_bbc %>% 
  dplyr::filter(freq  >= 7 ) %>%
  ggplot(aes(x = word, y = freq)) +
  geom_col()
```

```{r, results='asis'}
df_nyt %>% 
  dplyr::filter(freq  >= 7 ) %>%
  ggplot(aes(x = word, y = freq)) +
  geom_col()
```

```{r, results='asis'}
df_cnn %>% 
  dplyr::filter(freq >= 7 ) %>%
  ggplot(aes(x = word, y = freq)) +
  geom_col()
```

```{r, results='asis'}
df_grd %>% 
  dplyr::filter(freq >= 7 ) %>%
  ggplot(aes(x = word, y = freq)) +
  geom_col()
```

The graphs did not display particular differences in the usage of the most common words: all the articles had the words *Italy* and *Meloni*, whereas some shared also other words more linked with the politics field like *party* and *government*.

## SENTIMENT ANALYSIS

For the sentiment analysis I used overall two packages: `tidytext` and `transforEmotion`.

As first thing I wanted to understand if the general sentiment was positive or negative. To accomplish this, I had to choose the method and the vocabulary to evaluate the emotions present in the text. I chose the Bing Liu and collaborators corpus, which is already provided in the package `tidytext` and which assigns a positive or negative value for each word present in the dictionary.

```{r}
positive <- get_sentiments("bing") %>%
  filter(sentiment == "positive")

pos_text <- df %>%
  semi_join(positive) %>%
  count(word, sort = TRUE)
#84 words 

#GET THE NEGATIVE
negative <- get_sentiments("bing") %>%
  filter(sentiment == "negative")

neg_text <- df %>%
  semi_join(negative) %>%
  count(word, sort = TRUE)
#80 words 
```

To get the negative and positive words, I first extrapolated from the Bing dictionary all the words with a positive or negative meaning and then I extracted these words from the journal texts to see how many positive or negative words there were. The results show that in our corpus there are 84 positive words against 80 negative words. As a consequence, since there is no huge difference between positive and negative words, the sentiment of the article can be considered quite neutral.

To visually display the differences of the positive and negative words, i want to create a data frame also with the column "source" adding from which article the words come from:

```{r}
df_bbc$source <- ifelse(df_bbc$freq > 0, "BBC", df_bbc$source)
df_cnn$source <- ifelse(df_cnn$freq > 0, "CNN", df_cnn$source)
df_grd$source <- ifelse(df_grd$freq > 0, "GRD", df_grd$source)
df_nyt$source <- ifelse(df_nyt$freq > 0, "NYT", df_nyt$source)

#I put them in a single dataframe
complete_dat <- data.frame(
  word = c(df_bbc$word, df_cnn$word, df_grd$word, df_nyt$word), 
  freq = c(df_bbc$freq, df_cnn$freq, df_grd$freq, df_nyt$freq),
  source = c(df_bbc$source, df_cnn$source, df_grd$source, df_nyt$source)
)
complete_dat
```

Then, I add the sentiment to each word:

```{r}
bing <- get_sentiments("bing")

xxxxxxxx <- complete_dat %>%
  inner_join(bing) %>%
  count(word, sentiment, sort = TRUE)
```

At this point I created a visualization of how each word influenced the sentiment

```{r, results='asis'}
xxxxxxxx %>%
  filter(n >= 2) %>%
  mutate(n = ifelse(sentiment == "negative", -n, n)) %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(word, n, fill = sentiment)) +
  geom_col() +
  coord_flip() +
  labs(y = "Contribution to sentiment")
```

By a quick overview of the previous graph, we can notice that there are more positive words than the negative ones and that their impact on the contribution to the sentiment is higher.

Most of the the words that arise the level of positivity of the text are linked with the theme of the victory: in fact, we can find terms like *victory*, *won*, *winning*, *win*. But we can also notice a mistake: in the list of positive words that have a high impact on the sentiment there is the term *right*, which is perceived as positive if we consider *right* as the opposite of *wrong*, but in these articles, this word is referred to the political tendency of the party. So it would be better to add this word to the stop words to be removed for the analysis.

Among the words that have a higher negative impact there are words like *betray* and *sad*, but it is interesting that terms linked with the fascism appear more than once, like *fascism* itself and *fascist*.

In the following graph you can have a zoom on the words that impacted the most on the positive and negative sentiment:

```{r, results='asis'}
xxxxxxxx %>%
  group_by(sentiment) %>%
  slice_max(n, n = 2) %>% 
  ungroup() %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(n, word, fill = sentiment)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~sentiment, scales = "free_y") +
  labs(x = "Contribution to sentiment",
       y = NULL)
```

At this point I wanted to take the analysis a step further and see the specific emotions displayed in each text, going beyond the simple distinction between positive and negative. To do so, I used the package `transforEmotion`. This useful tool allowed me to compare the texts with the lexicon developed by Araque, Gatti, Staiano, and Guerini (2018), which assigns emotions to more than 175'000 words.

```{r}
ref <- data("emotions")
view(data)
vector <- c(uni_BBC, uni_CNN, uni_Grd, uni_NYT)
emotions <- emoxicon_scores(text = vector, lexicon = emotions)
#Now I can see the emotions associated with each text
view(emotions)
```

If we have a look at the data frame `emotions` we can notice that the emotion that are more present in the texts are inspiration, fear, angry and amusement.

## CONCLUSION

The results of this short analysis partially met my initial expectations. In fact, I thought the most prominent sentiment would have been negative. Yet, the results displayed a quite neutral sentiment, since the positive and negative words were almost balanced. Nevertheless, we must consider that most of the positive words were synonyms (*win*, *won*, *victory*, *winning*), and those terms cannot be avoided when you write an article about the results of the political campaigns. Instead, the negative words had a wider spectrum, and the only term that is repeated is *fascism*, which is a very negative image to associate to a political party in 2022.

It is also important to remind that this sentiment analysis was word-based, so the results might have been different if we operated a sentence-based analysis, since it can happen that a positive word is preceded by a negation, making it a negative word.

## DIFFICULTIES

I found this project very interesting because it gave me the opportunity to learn so much about sentiment analysis, which is a feature I think would be useful in the working world. This work was stimulating on many different levels, but there are some challenges that I did not manage to overcome.

First, when I scraped the texts from the internet, they were split into many sentences and I wanted them to be a single text. I tried to unify them using a loop, but it did not work out, so I need to unify them one by one, which is neither elegant nor practical.

Secondly, I wanted to create a Venn Diagram to display the most frequent words of each article, so that we could also see the overlaps. I tried to do so but at the end I discovered that with the package VennDiagram you can only display numbers and not words.

## REFERENCES

J. Silge, D. Robinson, *"Text Mining with R: a Tidy Approach"*, O'Reilley, 1st edition
